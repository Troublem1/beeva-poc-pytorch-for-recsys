{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems with Spotlight (on Pytorch)\n",
    "\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: | ^C\n",
      "failed\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "print(1)\n",
    "import sys\n",
    "!conda clean --index-cache\n",
    "!conda install --yes --prefix {sys.prefix} -c maciejkula -c pytorch spotlight=0.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs: \n",
      "    - tqdm\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    tqdm-4.19.5                |             py_0          32 KB  conda-forge\n",
      "    ca-certificates-2018.1.18  |                0         140 KB  conda-forge\n",
      "    certifi-2018.1.18          |           py27_0         143 KB  conda-forge\n",
      "    openssl-1.0.2n             |                0         3.5 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    tqdm:            4.19.5-py_0           conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2017.08.26-h1d4fec5_0             --> 2018.1.18-0      conda-forge\n",
      "    certifi:         2018.1.18-py27_0                  --> 2018.1.18-py27_0 conda-forge\n",
      "    openssl:         1.0.2n-hb7f436b_0                 --> 1.0.2n-0         conda-forge\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "tqdm 4.19.5: ########################################################### | 100% \n",
      "ca-certificates 2018.1.18: ############################################# | 100% \n",
      "certifi 2018.1.18: ##################################################### | 100% \n",
      "openssl 1.0.2n: ######################################################## | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install --yes -c conda-forge tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spotlight.cross_validation import random_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide matrix vs. narrow matrix.\n",
    "\n",
    "#### Using dataset Movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on precision_recall_score\n",
    "from spotlight.evaluation import precision_recall_score\n",
    "import numpy as np\n",
    "\n",
    "def f1_score(model, test):\n",
    "    pre_rec  = precision_recall_score(model, test)\n",
    "    precision = pre_rec[0]\n",
    "    recall = pre_rec[1]\n",
    "    f1 = [2*precision[i]*recall[i]/(0.0001+precision[i]+recall[i]) for i in range(len(precision))]\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import pdb\n",
    "def map_score(model, test, train=None):\n",
    "    \"\"\"\n",
    "    Compute mean average precision (MAP) scores.\n",
    "    then computes the resultant mean for all users.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    model: fitted instance of a recommender model\n",
    "        The model to evaluate.\n",
    "    test: :class:`spotlight.interactions.Interactions`\n",
    "        Test interactions.\n",
    "    train: :class:`spotlight.interactions.Interactions`, optional\n",
    "        Train interactions. If supplied, scores of known\n",
    "        interactions will be set to very low values and so not\n",
    "        affect the MAP.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    map score: numpy array of shape (num_users,)\n",
    "        Array of MAP scores for each user in test.\n",
    "    \"\"\"\n",
    "\n",
    "    test = test.tocsr()\n",
    "    if train is not None:\n",
    "        train = train.tocsr()\n",
    "\n",
    "    ap = []\n",
    "    import pdb\n",
    "    for user_id, row in enumerate(test):\n",
    "        if not len(row.indices):\n",
    "            continue\n",
    "        predictions = -model.predict(user_id)\n",
    "        if train is not None:\n",
    "            pdb.set_trace()\n",
    "            predictions[train[user_id].indices] = FLOAT_MAX\n",
    "        prec = []\n",
    "        #ranking = np.sort(st.rankdata(predictions)[row.indices])\n",
    "        ranking = predictions[row.indices]\n",
    "        num_hits = 0.0\n",
    "        score = 0.0\n",
    "        for index, value in enumerate(ranking):\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (row.indices[index]+1.0)\n",
    "            #prec.append((index + 1) / value)\n",
    "        prec.append(score)\n",
    "        ap.append(sum(prec) / len(ranking))\n",
    "    return np.array(ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La métrica MAP ha sido sacada de https://www.kaggle.com/c/avito-prohibited-content/discussion/9584 . Basado en la implementación de average precision de https://www.kaggle.com/hardyce/testing-mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def average_precision(relevant_items, recommended, recomm_length):\n",
    "    \"\"\"\n",
    "    Calculates the average precision for the specified recommendation_length.\n",
    "    This function computes the average precision at recommendation_length between two lists of\n",
    "    items.\n",
    "        :type relevant_items: list\n",
    "    :param relevant_items: Relevant items for the user\n",
    "    :type recommended: list\n",
    "    :param recommended: Items recommended for the user\n",
    "    :type recomm_length: int\n",
    "    :param recomm_length: Length of the recommendation\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    if len(recommended)>recomm_length:\n",
    "        recommended = recommended[:recomm_length]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i,p in enumerate(recommended):\n",
    "        if p in relevant_items and p not in recommended[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "    if not relevant_items:\n",
    "        return 0.0\n",
    "    return score / min(len(relevant_items), recomm_length)\n",
    "\n",
    "def get_relevant_items(user, threshold, actions):\n",
    "    \"\"\"\n",
    "    Gets the\n",
    "    :param user: User to obtain relevant items\n",
    "    :type user: str\n",
    "    :param threshold: Threshold to separate relevant from non relevant\n",
    "    :type threshold: float\n",
    "    :rtype: ndarray\n",
    "    \"\"\"\n",
    "    user_actions = actions[actions['user_id'] == user]\n",
    "    relevant_user_items = user_actions[user_actions['value'] >= threshold]['item_id'].values.tolist()\n",
    "    return relevant_user_items\n",
    "\n",
    "def mean_average_precision(recommendations, users, threshold, actions, at):\n",
    "    \"\"\"\n",
    "    Calculates the mean average precision\n",
    "    :type recommendations: list\n",
    "    :param recommendations: List of recommendations\n",
    "    :type users: list\n",
    "    :param users: Users for MAP\n",
    "    :type threshold: float\n",
    "    :param threshold: Relevant items threshold\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    print \"Average Precission progress\"\n",
    "    for recommend in tqdm(recommendations):\n",
    "        user_relevant_items = get_relevant_items(recommend.get('user'), threshold, actions)\n",
    "        ap = average_precision(user_relevant_items, recommend.get('recommendation'),at)\n",
    "        average_precisions.append(ap)\n",
    "    return np.mean(average_precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aproximación 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada usuario recomendar los items con mayor ranking y posteriormente calcular la precisión de estas recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "DATASET = '1M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = get_movielens_dataset(variant=DATASET)\n",
    "train, test = random_train_test_split(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished on 84.8973610401s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model = ImplicitFactorizationModel(n_iter=3, loss='bpr', use_cuda=USE_GPU)\n",
    "model.fit(train)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training finished on {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métrica Ramiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP is 0.0330083207166\n",
      "f1_score is 0.0558900347804\n",
      "Evaluation finished on 24.4335558414s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "my_map_score = map_score(model, test)\n",
    "print(\"MAP is {}\".format(np.mean(my_map_score)))\n",
    "\n",
    "my_f1_score = f1_score(model, test)\n",
    "print(\"f1_score is {}\".format(np.mean(my_f1_score)))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Evaluation finished on {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métrica Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6036/6036 [01:04<00:00, 94.13it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "prediction_dict = {}\n",
    "for item_id in tqdm(np.unique(test.user_ids)):\n",
    "    prediction = model.predict(item_id)\n",
    "    score_list = []\n",
    "    for index, rating in enumerate(prediction.tolist()):\n",
    "        score_list.append({'rating': rating, 'user_id': index})\n",
    "    prediction_dict[item_id] = list(map(lambda x:x['user_id'],sorted(score_list, key=lambda x:x['rating'], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6036/6036 [00:18<00:00, 335.16it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dict = {}\n",
    "import pdb\n",
    "for item_id in tqdm(np.unique(test.user_ids)):\n",
    "    ratings_list = []\n",
    "    for index, user_id in enumerate(test.item_ids[test.user_ids == item_id]):\n",
    "        ratings_list.append({'user_id': user_id, 'rating': test.ratings[test.user_ids == item_id][index]})\n",
    "    test_dict[user_id] = list(map(lambda x:x['user_id'], sorted(ratings_list, key=lambda x: x['rating'], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6036/6036 [04:53<00:00, 20.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@6040 is 0.0462438723141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "average_precisions = []\n",
    "import pdb\n",
    "k= 6040\n",
    "for item_id, users_list in tqdm(prediction_dict.items()):\n",
    "    if test_dict.get(item_id):\n",
    "        average_precisions.append(average_precision(test_dict[item_id],prediction_dict[item_id],k))\n",
    "print(\"MAP@{} is {}\".format(k,np.mean(average_precisions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aproximación 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transponer la matriz de usuarios-items, para entrenar el sistema y para cada item recomendar los usuarios que mejor han valorado ese item. Posteriormente calcular la precisión de esta aproximación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spotlight import interactions\n",
    "d2 = interactions.Interactions(d1.item_ids, d1.user_ids, d1.ratings,d1.timestamps,d1.weights,d1.num_items,d1.num_users)\n",
    "train2, test2 = random_train_test_split(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished on 85.9191660881s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model2 = ImplicitFactorizationModel(n_iter=3, loss='bpr', use_cuda=USE_GPU)\n",
    "model2.fit(train2)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training finished on {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métrica Ramiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP is 0.0103377607946\n",
      "f1_score is 0.0362460288795\n",
      "Evaluation finished on 18.3569450378s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "my_map_score = map_score(model2, test2)\n",
    "print(\"MAP is {}\".format(np.mean(my_map_score)))\n",
    "\n",
    "my_f1_score = f1_score(model2, test2)\n",
    "print(\"f1_score is {}\".format(np.mean(my_f1_score)))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Evaluation finished on {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métrica Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3468/3468 [00:37<00:00, 92.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "prediction_dict2 = {}\n",
    "for item_id in tqdm(np.unique(test2.user_ids)):\n",
    "    prediction = model2.predict(item_id)\n",
    "    score_list = []\n",
    "    for index, rating in enumerate(prediction.tolist()):\n",
    "        score_list.append({'rating': rating, 'user_id': index})\n",
    "    prediction_dict2[item_id] = list(map(lambda x:x['user_id'],sorted(score_list, key=lambda x:x['rating'], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3468/3468 [00:18<00:00, 191.59it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dict2 = {}\n",
    "import pdb\n",
    "for item_id in tqdm(np.unique(test2.user_ids)):\n",
    "    ratings_list = []\n",
    "    for index, user_id in enumerate(test2.item_ids[test2.user_ids == item_id]):\n",
    "        ratings_list.append({'user_id': user_id, 'rating': test2.ratings[test2.user_ids == item_id][index]})\n",
    "    test_dict2[user_id] = list(map(lambda x:x['user_id'], sorted(ratings_list, key=lambda x: x['rating'], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3468/3468 [06:00<00:00,  9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@6040 is 0.0317547924938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "average_precisions = []\n",
    "import pdb\n",
    "k= 6040\n",
    "for item_id, users_list in tqdm(prediction_dict2.items()):\n",
    "    if test_dict2.get(item_id):\n",
    "        average_precisions.append(average_precision(test_dict2[item_id],prediction_dict2[item_id],k))\n",
    "print(\"MAP@{} is {}\".format(k,np.mean(average_precisions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Aproximación 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar recomendaciones de los items para usuarios, agrupar posteriormente para cada item los usuarios que lo han votado y calcular su precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spotlight.cross_validation import random_train_test_split\n",
    "from spotlight.datasets.movielens import get_movielens_dataset\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_GPU = False\n",
    "DATASET = '1M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3 = get_movielens_dataset(variant=DATASET)\n",
    "train3, test3 = random_train_test_split(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished on 70.0328481197s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "model3 = ImplicitFactorizationModel(n_iter=3, loss='bpr', use_cuda=USE_GPU)\n",
    "model3.fit(train3)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training finished on {}s\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métrica Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6035/6035 [01:03<00:00, 95.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 6040 is out of bounds for axis 0 with size 6040\n",
      "0\n",
      "6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "from tqdm import tqdm\n",
    "predict_dict_users3 = {}\n",
    "predict_dict_items3 = {}\n",
    "for user_id in tqdm(np.unique(test3.user_ids)):\n",
    "    try:\n",
    "        prediction = model3.predict(user_id)\n",
    "        predict_dict_users3[str(user_id)] = prediction.tolist()\n",
    "        prediction_dict3 = {}\n",
    "        for item_id, score in enumerate(prediction):\n",
    "            if predict_dict_items3.get(str(item_id)) is None:\n",
    "                #predict_dict_items[str(item_id)] = []\n",
    "                predict_dict_items3[str(item_id)] = np.empty(6040,)\n",
    "            #predict_dict_items[str(item_id)].append({'r':score, 'u': user_id})\n",
    "            np.put(predict_dict_items3[str(item_id)], user_id, score)\n",
    "    except Exception as e:\n",
    "        print e\n",
    "        print item_id\n",
    "        print user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3707/3707 [00:28<00:00, 130.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "prediction_dict = {}\n",
    "for item_id, values in tqdm(predict_dict_items3.items()):\n",
    "    score_list = []\n",
    "    for index, rating in enumerate(values.tolist()):\n",
    "        score_list.append({'rating': rating, 'user':index})\n",
    "    prediction_dict3[item_id] = list(map(lambda x:x['user'],sorted(score_list, key=lambda x:x['rating'], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3487/3487 [00:18<00:00, 193.17it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dict3 = {}\n",
    "import pdb\n",
    "for item_id in tqdm(np.unique(test3.item_ids)):\n",
    "    ratings_list = []\n",
    "    for index, user_id in enumerate(test3.user_ids[test3.item_ids == item_id]):\n",
    "        ratings_list.append({'user': user_id, 'rating': test3.ratings[test3.item_ids == item_id][index]})\n",
    "    test_dict3[item_id] = list(map(lambda x:x['user'], sorted(ratings_list, key=lambda x: x['rating'], reverse=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@6040 is 0.0371521970202\n"
     ]
    }
   ],
   "source": [
    "average_precisions = []\n",
    "k = 6040\n",
    "for items, users in test_dict3.items():\n",
    "    average_precisions.append(average_precision(test_dict3[items],prediction_dict3[str(items)],k))\n",
    "print(\"MAP@{} is {}\".format(k,np.mean(average_precisions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
